{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised Learning: Regression Models and Performance Metrics"
      ],
      "metadata": {
        "id": "1FlqMPdx51Ka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression (SLR)? Explain its purpose.\n",
        "\n",
        "   -> Simple Linear Regression (SLR) is a statistical method used to study the relationship between two variables — one independent variable (X) and one dependent variable (Y) — by fitting a straight line to the observed data.\n",
        "\n",
        "###Purpose of simple linear regression\n",
        "\n",
        "- **Understanding relationships:** SLR helps to determine if a relationship exists between two continuous variables and to describe the nature of that relationship (e.g., positive or negative).\n",
        "\n",
        "- **Quantifying impact:** It provides a way to measure the strength of the relationship, estimating how much the dependent variable is expected to change for a one-unit change in the independent variable.\n",
        "\n",
        "- **Prediction:** By finding the \"best-fit\" straight line, SLR can be used to make predictions about the dependent variable for new values of the independent variable.\n",
        "\n",
        "- **Foundation for advanced models:** It serves as a fundamental building block for more complex algorithms in machine learning.\n",
        "\n",
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "   -> The key assumptions of simple linear regression are linearity (the relationship between variables is a straight line), independence of errors (errors are not correlated), homoscedasticity (the variance of errors is constant), and normality of residuals (errors are normally distributed). For models with more than one predictor, there is also an assumption of no multicollinearity.\n",
        "\n",
        "3. Write the mathematical equation for a simple linear regression model and\n",
        "explain each term.\n",
        "\n",
        "   -> The mathematical equation for a simple linear regression model is\n",
        "   \n",
        "                      𝑌𝑖=𝛽0+𝛽1𝑋𝑖+𝜀𝑖\n",
        "\n",
        "In this equation, 𝑌𝑖 represents the dependent variable, which is the value we want to predict or explain. The term 𝑋𝑖 is the independent variable, the known factor used to predict 𝑌𝑖. The coefficient 𝛽0 is called the intercept; it shows the expected value of 𝑌 when 𝑋 is zero, essentially indicating where the regression line crosses the Y-axis. The coefficient β1 is the slope of the line, showing how much 𝑌 changes on average for a one-unit increase in 𝑋. The term 𝜀 𝑖 represents the error or residual, which is the difference between the actual observed value and the value predicted by the regression line. It captures the influence of all other factors that affect 𝑌 but are not included in the model.\n",
        "\n",
        "4. Provide a real-world example where simple linear regression can be\n",
        "applied.\n",
        "\n",
        "   -> A simple linear regression can be applied to predict house prices based on house size. For example, if larger houses generally sell for more, a regression model can estimate how much the price increases for each additional square foot of area.\n",
        "\n",
        "5. What is the method of least squares in linear regression?\n",
        "\n",
        "   -> The method of least squares in linear regression is a technique used to find the best-fitting line through a set of data points by minimizing the sum of the squared differences between the observed values and the values predicted by the line.\n",
        "\n",
        "In other words, it chooses the line that makes the errors (residuals) as small as possible. Mathematically, it minimizes this quantity.\n",
        "\n",
        "6. What is Logistic Regression? How does it differ from Linear Regression?\n",
        "\n",
        "   -> Logistic regression is a statistical technique used when the outcome or dependent variable is categorical, usually taking only two possible values such as “yes” or “no,” “1” or “0.” Rather than predicting an exact numerical value like linear regression, logistic regression predicts the probability that an observation belongs to a particular category. It uses the logistic (sigmoid) function to ensure that the predicted probabilities always fall between 0 and 1.\n",
        "\n",
        "The main difference between logistic and linear regression lies in what they predict and how they interpret the relationship between variables. Linear regression models a straight-line relationship and can produce any real number as output, making it suitable for continuous outcomes like income or temperature. Logistic regression, on the other hand, models the relationship between the predictors and the log-odds of the outcome, not the outcome itself. This transformation allows it to handle classification tasks—such as determining whether an email is spam or not—by converting predicted probabilities into categories.\n",
        "\n",
        "7. Name and briefly describe three common evaluation metrics for regression\n",
        "models.\n",
        "\n",
        "  -> Three common evaluation metrics for regression models are Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared.\n",
        "\n",
        "Mean Absolute Error (MAE) measures the average absolute difference between the predicted and actual values. It shows how far predictions are, on average, from the true outcomes, with smaller values indicating better accuracy.\n",
        "\n",
        "Mean Squared Error (MSE) is the average of the squared differences between predicted and actual values. By squaring the errors, it penalizes larger mistakes more heavily, making it useful when big errors are particularly undesirable.\n",
        "\n",
        "R-squared , or the coefficient of determination, represents the proportion of the variance in the dependent variable that is explained by the model. Its value ranges from 0 to 1, where values closer to 1 indicate that the model explains most of the variation in the data.\n",
        "\n",
        "8. What is the purpose of the R-squared metric in regression analysis?\n",
        "\n",
        "  -> The purpose of the R-squared (𝑅2 ) metric in regression analysis is to measure how well the independent variable(s) explain the variation in the dependent variable. It represents the proportion of the total variance in the observed data that is accounted for by the regression model.\n",
        "\n",
        "An 𝑅2 value of 0 means the model does not explain any of the variation, while a value of 1 means the model explains all the variation perfectly. In other words, 𝑅2 provides an indication of the goodness of fit of the model: the higher the value, the better the model captures the patterns in the data.\n",
        "\n",
        "It's important to note, however, that a high 𝑅2 does not guarantee that the model is appropriate—it only measures how much of the variation is explained, not whether the predictions are unbiased or whether the model assumptions hold.\n",
        "\n",
        "9. Write Python code to fit a simple linear regression model using scikit-learn\n",
        "and print the slope and intercept.\n"
      ],
      "metadata": {
        "id": "mr8s3fPK6WYU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WzDpbQZ5hMK",
        "outputId": "883208cf-9b04-4e80-feae-83de01d32985"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slope (β1): 0.6\n",
            "Intercept (β0): 2.2\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # Independent variable\n",
        "Y = np.array([2, 4, 5, 4, 5])                # Dependent variable\n",
        "\n",
        "# Create and fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, Y)\n",
        "\n",
        "# Print slope and intercept\n",
        "print(\"Slope (β1):\", model.coef_[0])\n",
        "print(\"Intercept (β0):\", model.intercept_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How do you interpret the coefficients in a simple linear regression model?\n",
        "\n",
        "     -> In a simple linear regression model, the coefficients represent the relationship between the independent variable 𝑋 and the dependent variable 𝑌. The intercept (𝛽0) indicates the expected value of 𝑌 when 𝑋 is zero; it is the point where the regression line crosses the Y-axis. The slope (𝛽1) shows the expected change in 𝑌 for a one-unit increase in 𝑋.\n",
        "\n",
        "     For example, if the slope is 3, it means that for every one-unit increase in 𝑋, 𝑌 is expected to increase by 3 units. Similarly, a negative slope would indicate that 𝑌 decreases as 𝑋 increases. Together, these coefficients quantify both the direction and strength of the linear relationship between the predictor and the response variable."
      ],
      "metadata": {
        "id": "72s5BJyWGm25"
      }
    }
  ]
}